[{"content":"Introduction Cloudwatch is an integral part of the AWS ecosystem. Every service in AWS reports to cloudwatch for the service logs, application logs, metrics, etc.\nIn this article let\u0026rsquo;s discuss the cloudwatch custom metrics in detail.\nMetrics help us with finding the performance of the AWS services and the applications we run using these services. It also allows us to visualize the data with graphs and dashboards and create alarms based on the data reported tho metrics. If you are new to cloudwatch and cloudwatch metrics you can learn the basic concepts here\nCustom Metrics By default, AWS provides free metrics for most of its services. Apart from its own service metrics, AWS allows us to publish custom metrics, which means we can send our application-specific metrics to cloudwatch metrics. for example, we can push the metrics for the duration of third-party api calls, or the count of status codes returned by an API, etc. Then we can create alarms, dashboards based on those metrics.\nNow let\u0026rsquo;s see how we can create custom metrics and put data points to them, There are three ways of creating custom cloudwatch metrics from your application.\n AWS API\u0026rsquo;s/SDK for cloudwatch metric Metric Log Filters Cloudwatch Embedded Metric Format  Let\u0026rsquo;s see how we can create Custom metrics with the above three methods. For the demo purpose, let\u0026rsquo;s assume we have an AWS lambda function that calls a weather API, and we want to create metrics around the API call duration and the count of status codes returned by the API endpoint.\nAWS API\u0026rsquo;s/SDK\u0026rsquo;s This method uses the AWS cloudwatch metrics SDK\u0026rsquo;s putMetricData  API to create the custom metrics. This method is pretty straightforward, but the problem with this method is that it will incur an additional API call and it can block other API calls in your application while putting metrics to cloudwatch. This could affect the latency of your application (for eg: REST APIs). Also, each putMetricData api call involves cost. AWS will charge $0.01 per 1000 requests.\nExample\n'use strict'; const axios = require('axios') const AWS = require('aws-sdk') const cloudwatch = new AWS.CloudWatch(); module.exports.handler = async (event) =\u0026gt; { try { const startTime = new Date() const response = await axios.get('https://www.metaweather.com/api/location/2487956/2021/8/8') const apiStatusCode = response.status const endTime = new Date() console.log(apiStatusCode) const apiCallDuration = endTime - startTime const statusMetricParams = { MetricData: [ { MetricName: 'status_code', Dimensions: [ { Name: 'status_code', Value: `http_${apiStatusCode}` }, ], Timestamp: new Date(), Unit: 'Count', Value: 1, } ], Namespace: 'MetricFromSDK_1' }; await cloudwatch.putMetricData(statusMetricParams).promise(); const durationMetricParams = { MetricData: [ { MetricName: 'api_call_duration', Dimensions: [ { Name: 'api_name', Value: `location_api` }, ], Timestamp: new Date(), Unit: 'Milliseconds', Value: apiCallDuration, } ], Namespace: 'MetricFromSDK_1' }; await cloudwatch.putMetricData(durationMetricParams).promise(); } catch (error) { console.error('failed',error) } };  Metric Log Filters Metric log filters can search and filter data points needed to create metrics from Cloudwatch log groups. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. When you create a metric from a log filter, you can also choose to assign dimensions and a unit to the metric. If you specify a unit, be sure to specify the correct one when you create the filter. Changing the unit for the filter later will have no effect.\nWith this method, the metrics are generated asynchronously. You don\u0026rsquo;t need any additional API calls from the application to generate the metrics. You just need to log the metrics data in a JSON format in the application and create a metric filter for each metric on the applications cloudwatch log group which filters the metric data from the logs based on the filter expressions defined.\nThe only downside I see with this method is the creation of metric filters on log groups every time you need to create a new metric. You can create them manually or use any IaC tool to generate them on demand.\nExample\n'use strict'; const axios = require('axios') module.exports.handler = async (event) =\u0026gt; { try { const startTime = new Date() const response = await axios.get('https://www.metaweather.com/api/location/2487956/2021/8/8') const apiStatusCode = response.status const endTime = new Date() const apiCallDuration = endTime - startTime console.log({ metricName: 'api_call_duration', metricValue: apiCallDuration }) console.log({ metricName: 'status_code_count', metricValue: apiStatusCode}) console.log({[`http_${apiStatusCode}`]: 1}) } catch (error) { console.error(error) } };  Once the logs are pushed to cloudwatch logs, the next step is to create a metric filter on the log from which we want to filter the data points to generate the metric.\nFollow the below screenshots to see how to create a metric filter based on the logs that we generate from the code. Once the metric filter is created properly and the filter patterns match with the logs it will create a metric and start pushing data points to it on every new log.\nCloudwatch Embedded Metric Format The CloudWatch embedded metric format is a JSON specification used to instruct CloudWatch Logs to automatically extract metric values embedded in structured log events. You can use CloudWatch to graph and create alarms on the extracted metric values.\nThis is my personal favorite method. This is an asynchronous process, which means it does not make any API call to generate metrics, and no metric filters are needed. All you have to do is log your metrics to cloudwatch in a specific JSON format as documented here. AWS will automatically parse these logs from cloudwatch log groups and generate the metrics for you.\nThere are two ways to use this method,\n Directly log the metrics in JSON format as documented here Using embedded metric NPM module from AWS (Examples available at the modules GitHub page here)  Below is an example of the first method.\n'use strict'; const axios = require('axios') module.exports.handler = async (event) =\u0026gt; { try { const startTime = new Date() const response = await axios.get('https://www.metaweather.com/api/location/2487956/2021/8/') const apiStatusCode = response.status const endTime = new Date() console.log(apiStatusCode) const apiCallDuration = endTime - startTime // Create Metric For Status Code Count console.log( JSON.stringify({ message: '[Embedded Metric]', // Identifier for metric logs in CW logs status_code_count: 1, // Metric Name and value status_code: `http_${apiStatusCode}`, // Diamension name and value _aws: { Timestamp: Date.now(), CloudWatchMetrics: [ { Namespace: `demo_2`, Dimensions: [['status_code']], Metrics: [ { Name: 'status_code_count', Unit: 'Count', }, ], }, ], }, }) ) // Create Metric For API Call Duration console.log( JSON.stringify({ message: '[Embedded Metric]', // Identifier for metric logs in CW logs api_call_duration: apiCallDuration, // Metric Name and value api_name: 'location_api', // Diamension name and value _aws: { Timestamp: Date.now(), CloudWatchMetrics: [ { Namespace: `demo_2`, Dimensions: [['api_name']], Metrics: [ { Name: 'api_call_duration', Unit: 'Milliseconds', }, ], }, ], }, }) ) } catch (error) { console.error(error) } };  Below are the screenshots of the custom metrics we created with the above methods,\nName Space\nDimension\nMetric\nConclusion We have discussed three methods above, First one is synchronous and the other two are asynchronous. I personally prefer the asynchronous method because the metric generation process will not block the other API calls in the application.\nCloudwatch custom metrics can be used in the following scenarios,\n Third-Party integration metrics(API call duration, success or failed count of processes, etc) Custom metrics around events/processes in the application  ","permalink":"https://imewish.github.io/posts/cloudwath-custom-metrics-and-how-to-create-them/","summary":"Introduction Cloudwatch is an integral part of the AWS ecosystem. Every service in AWS reports to cloudwatch for the service logs, application logs, metrics, etc.\nIn this article let\u0026rsquo;s discuss the cloudwatch custom metrics in detail.\nMetrics help us with finding the performance of the AWS services and the applications we run using these services. It also allows us to visualize the data with graphs and dashboards and create alarms based on the data reported tho metrics.","title":"CloudWatch Custom Metrics And How To Create Them"},{"content":"Amazon Web Services released SQS triggers for Lambda functions in June 2018. You can use an AWS Lambda function to process messages in an Amazon Simple Queue Service (Amazon SQS) queue. Lambda polls the queue and invokes your Lambda function synchronously with an event that contains queue messages. Lambda reads messages in batches and invokes your function once for each batch. When your function successfully processes a batch, Lambda deletes its messages from the queue.\nHow Does Lambda Process The Messages? By default, Lambda invokes your function as soon as records are available in the SQS queue. Lambda will poll up to 10(Can be increased) messages in your queue at once and will send that batch to the function. This means each invocation of lambda will receive up to 10 messages from the Queue to process. Once all messages in the batch are processed lambda will delete the batch from the queue and will start processing the next batch.\nWhat Happens If One Of The Message In The Batch Failed To Process?  If you don\u0026rsquo;t throw an error on your function and one of the messages didn\u0026rsquo;t process correctly the message will be lost If you catch and throw an error, the whole batch will be sent back to the queue including the ones which were processed successfully. This batch will be retried again multiple times based on the maxReceiveCount configuration if the error is not resolved. This will lead to reprocessing of successful messages multiple times if you have configured a Dead letter Queue configured with your SQS Queue the failed batch will end up there once the ReceiveCount for a message exceeds the maxReceiveCount . The successfully processed messaged will also end up in the DLQ. If the consumer of this DLQ has the ability to differentiate between failure and success messages in the batch we are good to go.  How to Handle The Partial Failure?   Use a batchSize of 1\nThis is useful in low-traffic scenarios. Only one message will be sent to lambda on each invocation. But this limits the throughput of how quickly you are able to process messages\n  Delete successfully processed messages\nThis is the most effective method to handle this situation. Process the batch messages inside a try-catch block and store the receiptHandle for each message in an array and call the sq.deleteMessage API and delete those messages when you catch the error and throw an error once you delete the messages that are successfully processed.\n'use strict'; const AWS = require('aws-sdk') const sqs = new AWS.SQS(); module.exports.handler = async event =\u0026gt; { const sqsSuccessMessages = []; try { const records = event.Records ? event.Records : [event]; for (const record of records) { await processMessageFucntion(record) // Store successfully processed records sqsSuccessMessages.push(record); } } catch (e) { if (sqsSuccessMessages.length \u0026gt; 0) { await deleteSuccessMessages(sqsSuccessMessages); } throw new Error(e); } }; // Delete success messages from the queue incase any failure while processing the batch // On no failure case lambda will delete the whole batch once processed const deleteSuccessMessages = async messages =\u0026gt; { for (const msg of messages) { await sqs .deleteMessage({ QueueUrl: getQueueUrl({ sqs, eventSourceARN: msg.eventSourceARN }), ReceiptHandle: msg.receiptHandle }) .promise(); } }; const getQueueUrl = ({ eventSourceARN, sqs }) =\u0026gt; { const [, , , , accountId, queueName] = eventSourceARN.split(':'); return `${sqs.endpoint.href}${accountId}/${queueName}`; };    Conclusion\nAs of now, there is no support for handling partial batch failures in Lambda with SQS. It is totally up to you to decide if you want to handle it or not depends on your application need.\nRecently AWS had added support for custom checkpointing for DynamoDB Streams and Kinesis. This means customers now can automatically checkpoint records that have been successfully processed using a new parameter, FunctionResponseType. When customers set this parameter to Report Batch Item Failure, if a batch fails to process, only records after the last successful message are retried. This reduces duplicate processing and gives customers more options for failure handling.\nThis gives hope, that AWS may add something similar for SQS as wellüôè\n","permalink":"https://imewish.github.io/posts/aws-sqs-partial-failure-handling/","summary":"Amazon Web Services released SQS triggers for Lambda functions in June 2018. You can use an AWS Lambda function to process messages in an Amazon Simple Queue Service (Amazon SQS) queue. Lambda polls the queue and invokes your Lambda function synchronously with an event that contains queue messages. Lambda reads messages in batches and invokes your function once for each batch. When your function successfully processes a batch, Lambda deletes its messages from the queue.","title":"AWS SQS With Lambda, Partial Batch Failure Handling"},{"content":"The easiest and most common way of adding application configurations(Eg: feature toggle flags, secrets, fallback URLs, etc) with your serverless applications is by setting them as lambda environment variables. These variables are set to the lambda functions from a configuration file in your code (eg: serverless.yml) or read from secrets manager or parameter store etc and exported during the deployment on your CICD pipeline.\nThe problem with this approach is, suppose you have a serverless application that has multiple lambda functions under it. These lambda functions are set to do individual tasks. For eg: lambda1 is set to call a third-party payment service, and it reads the API URLs and Keys from lambda environment variables. Now if we want to change the API URL or KEY for this service, would result in a significantly longer and more involved build, test, and deployment process. Each and every time you make a change in the configuration you have to repeat the build, test, deploy process.\nIn this article, we will discuss how to decouple your application configuration from your application code and how to deploy the changes to the service without redeploying the application code base every time there is a change in the configurations. with AWS AppConfig.\nWhat is AppConfig? AWS AppConfig can be used to create, manage, and quickly deploy application configurations. AppConfig supports controlled deployments to applications of any size and includes built-in validation checks and monitoring. You can use AppConfig with applications hosted on EC2 instances, AWS Lambda, containers, mobile applications, or IoT devices.\nAWS AppConfig helps simplify the following tasks:\n  Configure\nSource your configurations from Amazon Simple Storage Service (Amazon S3), AWS AppConfig hosted configurations, Parameter Store, Systems Manager Document Store. Use AWS CodePipeline integration to source your configurations from Bitbucket Pipelines, GitHub, and AWS CodeCommit.\n  Validate\nWhile deploying application configurations, a simple typo could cause an unexpected outage. Prevent errors in production systems using AWS AppConfig validators. AWS AppConfig validators provide a syntactic check using a JSON schema or a semantic check using an AWS Lambda function to ensure that your configurations deploy as intended. Configuration deployments only proceed when the configuration data is valid.\n  Deploy and monitor\nDefine deployment criteria and rate controls to determine how your targets receive the new configuration. Use AWS AppConfig deployment strategies to set deployment velocity, deployment time, and bake time. Monitor each deployment to proactively catch any errors using AWS AppConfig integration with Amazon CloudWatch Events. If AWS AppConfig encounters an error, the system rolls back the deployment to minimize the ct on your application users.\n  AWS AppConfig can help you in the following use cases:\n Application tuning ‚Äì Introduce changes carefully to your application that can be tested with production traffic. Feature toggle ‚Äì Turn on new features that require a timely deployment, such as a product launch or announcement. Allow list ‚Äì Allow premium subscribers to access paid content. Operational issues ‚Äì Reduce stress on your application when a dependency or other external factor impacts the system.  DEMO\nThe generic way of using app config with your lambda function is to use AppConfig is using AppConfig SDK in the code and call the configuration. The problem with this approach is each and every lambda execution will call the AppConfig API\u0026rsquo;s which will incur additional costs and it might also hit the AppConfig service limits when the traffic is High.\nTo avoid calling the AppConfig API on each request Amazon has come up with a solution. They have created a Lambda Extension for AppConfig.\nWhen the AWS AppConfig extension starts, two main components are created:\nThe first, the proxy, exposes a localhost HTTP endpoint that can be called from your Lambda code to retrieve a piece of configuration data. The proxy does not call AWS AppConfig directly. Instead, it uses an extension-managed cache that contains the freshest configuration data available. Because the data is already available locally, the HTTP endpoint can be called on every invocation of your function (or even multiple times if you have long-running functions that you want to update mid-flight).\nThe second component, the retriever, works in the background to keep your configuration data fresh. It checks for potential updates even before your code asks for it. It tracks which configurations your function needs, whether the data is potentially stale, and makes appropriate calls to AWS AppConfig to retrieve fresher data, if available. It ensures the right metadata is passed to avoid any unnecessary data delivery and support various types of rollout strategy.\nThe determination of ‚Äúhow fresh is fresh‚Äù can be configured using Lambda environment variables. These configs changes rarely.\n AWS_APPCONFIG_EXTENSION_POLL_INTERVAL_SECONDS, which defaults to 45 seconds, specifies the frequency with which the extension checks for new configuration data. AWS_APPCONFIG_EXTENSION_POLL_TIMEOUT_MILLIS, which defaults to 3000 milliseconds, specifies the maximum time the extension waits for a piece of configuration data before giving up and trying again during the next poll interval AWS_APPCONFIG_EXTENSION_HTTP_PORT which defaults to 2772, specifies the port that the proxy‚Äôs HTTP endpoint uses  Now, Let\u0026rsquo;s create a simple REST API for the demo.\nFirst, we need to create a new application in AppConfig. For that go to AWS Console ‚Üí Systems Manager ‚Üí AppConfig\n Create an Application  Create an Environment  Create a Configuration Profile and add some configs  4. Deploy the configuration To the Code\nConsidering we have e-commerce API where you want to change the discounts for new customers, the value for discounts is something that can vary often. By Using AppConfig we can update that without any changes and deployments in our application code.\nBelow is the sample code for our demo App.\nhandler.js\nconst http = require('http'); const axios = require('axios') exports.demo = async (event) =\u0026gt; { let configData = await axios.get(\u0026quot;http://localhost:2772/applications/DemoApp/environments/develop/configurations/generalConfig\u0026quot;) let discountPercentage = configData.data.discountPercentage const response = { statusCode: 200, body: `You have ${discountPercentage}% off on your first purchase`, }; return response; };  http://localhost:2772/applications/DemoApp/environments/develop/configurations/generalConfig This URL is the HTTP endpoint for the proxy running on the lambda extension. Our Lambda will call this on every execution to get the latest configurations\nserverless.yml\nservice: appconfig-poc provider: name: aws runtime: nodejs12.x region: us-west-2 iamRoleStatements: - Effect: 'Allow' Action: - 'appconfig:GetConfiguration' Resource: '*' ## These are the lambda extension configurations environment: AWS_APPCONFIG_EXTENSION_POLL_INTERVAL_SECONDS: 30 AWS_APPCONFIG_EXTENSION_POLL_TIMEOUT_MILLIS: 3000 AWS_APPCONFIG_EXTENSION_HTTP_PORT: 2772 functions: demo: handler: handler.demo ## AWS AppConfig Lambda Layer ## Choose the layer for your region from here https://docs.aws.amazon.com/appconfig/latest/userguide/appconfig-integration-lambda-extensions.html layers: - arn:aws:lambda:us-west-2:359756378197:layer:AWS-AppConfig-Extension:18 events: - http: path: getDiscount method: get  Deploy the code. You will get an http endpoint.\n‚ûú sls deploy --stage dev Serverless: Running \u0026quot;serverless\u0026quot; installed locally (in service node_modules) Serverless: Packaging service... Serverless: Excluding development dependencies... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service appconfig-poc.zip file to S3 (135.58 KB)... Serverless: Validating template... Serverless: Updating Stack... Serverless: Checking Stack update progress... ............... Serverless: Stack update finished... Service Information service: appconfig-poc stage: dev region: us-west-2 stack: appconfig-poc-dev resources: 11 api keys: None endpoints: GET - https://xxxxxx.execute-api.us-west-2.amazonaws.com/dev/getDiscount functions: demo: appconfig-poc-dev-demo layers: None Serverless: Removing old service artifacts from S3...  Now once you call the endpoint you will get the message like this.\n‚ûú curl https://xxxxxx.execute-api.us-west-2.amazonaws.com/dev/getDiscount You have 5% off on your first purchase  Now change the value for discountPercentage on AppConfig and deploy.\n Goto configuration profile and create a new version of the configuration  2. Deploy the new version of the config\nOnce the deployment is finished hit the endpoint to see the updated discount percentage.\n‚ûú curl https://xxxxx.execute-api.us-west-2.amazonaws.com/dev/getDiscount You have 10% off on your first purchase  See We have successfully updated our application config without changing/deploying our codebase üéâ\nConclusion The demo above is a very simple use case of AWS AppConfig. But there are many other things we can achieve with it. AWS customers are using this for multiple use cases like,\n Feature flags: You can deploy features onto production that are hidden behind a feature flag. Toggling the feature flag turns on the feature immediately, without doing another code deployment. Allow lists: You might have some features in your app that are for specific callers only. Using AWS AppConfig, you can control access to those features and dynamically update access rules without another code deployment The verbosity of logging: You can control how often an event occurs by setting a variable limit. For example, you can adjust the verbosity of your logging during a production incident to better analyze what is going on. You would want to do another full deployment in the case of a production incident, but a quick configuration change gets you what you need.  ","permalink":"https://imewish.github.io/posts/decoupling-application-configuration-from-application-code-in-your-serverless-application-with-aws-appconfig/","summary":"The easiest and most common way of adding application configurations(Eg: feature toggle flags, secrets, fallback URLs, etc) with your serverless applications is by setting them as lambda environment variables. These variables are set to the lambda functions from a configuration file in your code (eg: serverless.yml) or read from secrets manager or parameter store etc and exported during the deployment on your CICD pipeline.\nThe problem with this approach is, suppose you have a serverless application that has multiple lambda functions under it.","title":"Decoupling Application configuration from application code in your serverless application with AWS Appconfig"},{"content":"Serverless is great, it helps companies to focus on product and application development without worrying much about the infrastructure and scaling. But there are some soft and hard limits for every AWS service which we need to keep in mind when we are developing a serverless application. These limits are set to protect the customer as well as the provider against any unintentional use.\nIn this article, we will talk about some of those limits and how to avoid them.\nDeployment Limits Lambda 1. 50 MB: Function Deployment Package Size, 250 MB: Size of code/dependencies that you can zip into a deployment package (uncompressed .zip/.jar size)\nThere is a limit of 50MB on the package size of the code which we upload to lambda.\nThis limit is applied when we try to create or update a lambda function with the AWS CLI.\nIf you try to create the function from the AWS Web console it is limited to 10MB. We can avoid these limitations by uploading the ZIP file to S3 and create the function.\nThe total size of code/dependencies we can compress into the deployment package is limited to 250MB. In simple REST API cases, we may not hit these limits. But when we have to use binaries like FFMPEG or ML/AI libraries like scikit-learn or ntlk with lambda could hit this limit. these dependencies are high in size\nAre these a soft limit? : NO\nHow to Avoid?\n- Use Serverless framework\nBy default serverless framework zip and deploys your code first to S3 and deploys it lambda via cloud formation.\n- Use WebPack\n*WebPack* is a well-known tool serving to create bundles of assets (code and files). Webpack helps to reduce and optimize the packaging size by\n Include only the code used by your function Optimize your NPM dependencies Use a single file for your source code  Optimizing and reducing the package size will also help to reduce the cold start of the functions.\n2. 75GB: Total Size Of All Deployment Packages That Can Be Uploaded Per Region\nThis limit is a region-wide soft limit. It can be increased by a service Quota limit increase. Most of the time people get hit by this limit is when they have a huge number of lambda functions and every time we update a new code a new version of lambda is created. Each version has its own deployment package it will be counted towards this limit.\nIs it a soft limit? : YES\nHow to Avoid?\n Version your code and do not version functions. (Except for lambda@edge, For lambda@edge versioning is a must) - Remove older or unused versions If you are updating the function via AWS CLI use --no-publish flag not to create a new version update**.** Keep only the latest version of the lambda function. Remove the older versions, and if we really needed to keep a specific older version of the function, add an ALIAS to those versions and remove all the unused versions.  3. 512MB: Amount of data that be stored inside lambda instance during execution (/tmp)\nIf you want to download a file and store in the /tmpdirectory to process it during the execution, this limit will be applied. You cannot store files into this directory only up to 512 MB, even if it is a single file or multiple files.\nIs it a soft limit? : NO\nHow to Avoid?\n Use the Nodejs Stream method to read and process and write files without loading the whole file into lambdas filesystem  4. 6MB: Lambda payload limit\nThis means we cannot POST more than 6MB of data to Lambda through API Gateway. So if we build an image or video uploading API to upload files to S3, we are limited to this 6MB Limit.\nIs it a soft limit? : NO\nHow to Avoid?\n- Use a pre-signed S3 URL.\nIn this case, the client makes an HTTP GET request to API Gateway, and the Lambda function generates and returns a pre-signed S3 URL, the client uploads the image to S3 directly, using the pre-signed S3 URL\nCloudformation If you are using the serverless framework for deploying your application, as your application grows you may hit some of the cloudformation limits when you deploy as serverless framework uses cloudformation behind the scenes for deploying services.\n A CloudFormation stack can have at most 500 resources  Let\u0026rsquo;s take an example of a backend Application with multiple REST API\u0026rsquo;s. This Application may have multiple Lambda functions, API Gateway Endpoints, Methods, Custom Domains, SNS Topics, DynamoDB, S3 Buckets, etc. When we deploy this application to AWS with cloudformation, it will create cloudformation resources for all the mentioned services in a single cloudformation stack. There will be multiple resources created per services(IAM roles, IAM Policies, Cloudwatch log groups). In the case of a single lambda function following resources will be created per each function,\n AWS::Lambda::Function AWS::Lambda::Version AWS::Logs::LogGroup  Plus additional resources will be added if we attach event-sources like API Gateway, SNS to the function When the application grows, the total number of resources will also increase. And when it hit the 500 limit the deployments will start failing.\nIs it a soft limit? : NO\nHow to Avoid?\n- Use Cloudformation Nested Stacks to Reuse Common Template Patterns,\nAs your infrastructure grows, common patterns can emerge in which you declare the same components in each of your templates. You can separate out these common components and create dedicated templates for them. That way, you can mix and match different templates but use nested stacks to create a single, unified stack. The nested solutions may buy you little time to avoid this limit, but as the stack grows it will be hard to manage it.\n- Use serverless split stack plugin\nThis plugin migrates CloudFormation resources into nested stacks in order to work around the 200 resource limit. There are built-in migration strategies that can be turned on or off as well as defining your own custom migrations.\nThe recommended way is, Try to create your services(Multiple Micro Services) as small as you can. Keep an eye on no resources every time you deploy the stack, and when you think the stack may hit the limit, break some of its features into alternative service.\nAn IAM role policy can have up to 10,240 characters  This is one of the other limits we may hit when the stack grows. This happens when the whole application uses a single IAM role. By default serverless will include all the basic and custom IAM policies for all the functions used by the application into one single IAM role.\nHow to Avoid?\n- Create individual IAM roles for each function in the cloudformation stack instead of a single large IAM role for the whole stack. Using per-function roles is a recommended best practice to achieve and maintain the least privilege setup for your Lambda functions.\n- With the serverless framework, there are a couple of good plugins that help to do this.\nSummary\nIt is a good practice to know all the limits of all the AWS services that you are going to use when designing your infrastructure and develop the application. This will help us with the following,\n- Avoid redesigning the architecture in the future when we hit the hard limit\n- Design scalable and fault-tolerant serverless infrastructure by planning and implementing workarounds to avoid hitting the limits or calculating and increasing the soft limit of each service as per the requirement of the application\n","permalink":"https://imewish.github.io/posts/aws-limits-to-keep-in-mind-while-developing-a-serverless-application/","summary":"Serverless is great, it helps companies to focus on product and application development without worrying much about the infrastructure and scaling. But there are some soft and hard limits for every AWS service which we need to keep in mind when we are developing a serverless application. These limits are set to protect the customer as well as the provider against any unintentional use.\nIn this article, we will talk about some of those limits and how to avoid them.","title":"AWS Service Limits To Keep In Mind While Developing A Serverless Application"},{"content":"Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don\u0026rsquo;t have to worry about hardware provisioning, setup, and configuration, replication, software patching, or cluster scaling. DynamoDB also offers encryption at rest, which eliminates the operational burden and complexity involved in protecting sensitive data.\nThis cheat sheet will cover the most commonly used scenarios of data operations in DynamoDB with AWS DynamoDB Document client for JavaScript/Nodejs. The DynamoDB Document Client is the easiest and most preferred way to interact with a DynamoDB database from a Nodejs or JavaScript application.\nGETTING STARTED Install npm install aws-sdk\nConfigure const AWS = require(\u0026#39;aws-sdk\u0026#39;) const ddb = new AWS.DynamoDB.DocumentClient() CREATE ITEM Let\u0026rsquo;s create a new item for the new user. This user will have one album and one image in the album.\nasync function createItem (buildInfo) { console.log(\u0026#39;Creating new item\u0026#39;) const params = { TableName: tableName, Item: { \u0026#39;userId\u0026#39;: \u0026#39;johnDoe\u0026#39;, \u0026#39;createdAt\u0026#39;: 1598362623, \u0026#39;updatedAt\u0026#39;: 1598362623, \u0026#39;albums\u0026#39;: { \u0026#39;album1\u0026#39;: { \u0026#39;id\u0026#39;: \u0026#39;album-kjuijhs342\u0026#39;, \u0026#39;createdAt\u0026#39;: 1598362623, \u0026#39;updatedAt\u0026#39;: 1598362623, \u0026#39;description\u0026#39;: \u0026#39;My First Album\u0026#39;, \u0026#39;Title\u0026#39;: \u0026#39;Holidays\u0026#39;, \u0026#39;images\u0026#39;: { \u0026#39;img-1\u0026#39;: { \u0026#39;filename\u0026#39;: \u0026#39;johndoe/album1/e8TtkC5xyv4.jpg\u0026#39;, \u0026#39;s3Url\u0026#39;: \u0026#39;s3://photo-bucket/johndoe/album1/e8TtkC5xyv4.jpg\u0026#39;, \u0026#39;tags\u0026#39;: [\u0026#39;nature\u0026#39;, \u0026#39;animals\u0026#39;] } } } } } } try { await ddb.put(params).promise() } catch (error) { console.log(error) } } SCAN Scan and returns all items in a table\nasync function scan() { const params = { TableName: tableName } try { await ddb.scan(params).promise() } catch (error) { console.error(error) } } GET ITEM Get a single item from the table\nasync function getItem() { const params = { TableName: tableName, Key: { \u0026#39;userId\u0026#39;: \u0026#39;johnDoe\u0026#39; } } try { await ddb.get(params).promise() } catch (error) { console.error(error) } } GET ONLY SOME DATA FROM AN ITEM this will return only the tags from img1 and img2 in the result.\nasync function getSome() { const params = { TableName: tableName, ProjectionExpression: `albums.album1.images.#imageName1.tags, albums.album1.images.#imageName2.tags`, ExpressionAttributeNames: { \u0026#39;#imageName1\u0026#39;: \u0026#39;img-1\u0026#39;, \u0026#39;#imageName2\u0026#39;: \u0026#39;img-2\u0026#39; }, Key: { \u0026#39;userId\u0026#39;: \u0026#39;johnDoe\u0026#39;, } } try { await ddb.get(params).promise() } catch (error) { console.error(error) } } DELETE ITEM deletes a single item from the table\nasync function deleteItem () { const params = { TableName: tableName, Key: { userId: \u0026#39;johnDoe\u0026#39;, } } try { await ddb.delete(params).promise() } catch (error) { console.error(error) } } QUERY Query an item from a table\nasync function query () { const params = { TableName: tableName, KeyConditionExpression: \u0026#39;userId = :id \u0026#39;, ExpressionAttributeValues: { \u0026#39;:id\u0026#39;: \u0026#39;johnDoe\u0026#39; } } try { await ddb.query(params).promise() } catch (error) { console.error(error) } } UPDATE A TOP-LEVEL ATTRIBUTE Let\u0026rsquo;s update the updatedAt key\nasync function updateItem () { const params = { TableName: tableName, Key: { userId: \u0026#39;johnDoe\u0026#39; }, UpdateExpression: \u0026#39;set updatedAt = :newUpdatedAt\u0026#39;, ExpressionAttributeValues: { \u0026#39;:newUpdatedAt\u0026#39;: 1598367687 }, ReturnValues: \u0026#39;UPDATED_NEW\u0026#39; } try { await ddb.update(params).promise() } catch (error) { console.error(error) } } UPDATE A NESTED ATTRIBUTE Here we will add a new attribute(size) to img-1 of album1\nasync function updateNestedAttribute() { const params = { TableName: tableName, Key: { userId: \u0026#39;johnDoe\u0026#39; }, UpdateExpression: `set albums.album1.images.#img.size = :newImage`, ConditionExpression: `attribute_not_exists(albums.album1.images.#img.size)`, // only creates if size attribute doestnt exists  ExpressionAttributeNames: { \u0026#39;#img\u0026#39;: \u0026#39;img-1\u0026#39; }, ExpressionAttributeValues: { \u0026#39;:newImage\u0026#39;: 2048 } } try { await ddb.update(params).promise() } catch (error) { console.error(error) } }  NOTE: If an attribute name begins with a number or contains a space, a special character, or a reserved word, then you must use an expression attribute name to replace that attribute\u0026rsquo;s name in the expression. In the above example, img-2 attribute has - in its name. So if we set the update expression to set albums.album1.images.image-2 = :newImage it will throw an error.\n APPEND TO A NESTED OBJECT Here we will add a new image to album1\nasync function appendToAnObject () { const newImage = { \u0026#39;filename\u0026#39;: \u0026#39;johndoe/album1/food-826349.jpg\u0026#39;, \u0026#39;s3Url\u0026#39;: \u0026#39;s3://photo-bucket/johndoe/album1/food-826349.jpg\u0026#39;, \u0026#39;tags\u0026#39;: [\u0026#39;burger\u0026#39;, \u0026#39;food\u0026#39;] } const params = { TableName: tableName, Key: { userId: \u0026#39;johnDoe\u0026#39; }, UpdateExpression: `set albums.album1.images.#image = :newImage`, ExpressionAttributeNames: { \u0026#39;#image\u0026#39;: \u0026#39;img-2\u0026#39; }, ExpressionAttributeValues: { \u0026#39;:newImage\u0026#39;: newImage } } try { await ddb.update(params).promise() } catch (error) { console.error(error) } } APPEND TO A LIST Here we will add a couple of tags to one of the images. Tags are stored as an array\nasync function appendToList() { const params = { TableName: tableName, Key: { userId: \u0026#39;johnDoe\u0026#39; }, UpdateExpression: \u0026#39;SET albums.album1.images.#image1.tags = list_append(albums.album1.images.#image1.tags, :newTags)\u0026#39;, ExpressionAttributeNames: { \u0026#39;#image1\u0026#39;: \u0026#39;img-1\u0026#39; }, ExpressionAttributeValues: { \u0026#39;:newTags\u0026#39;: [\u0026#39;burger\u0026#39;, \u0026#39;pizza\u0026#39;] } } try { await ddb.update(params).promise() } catch (error) { console.error(error) } } ","permalink":"https://imewish.github.io/posts/dynamodb-cheatsheet-for-nodejs-javascript/","summary":"Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don\u0026rsquo;t have to worry about hardware provisioning, setup, and configuration, replication, software patching, or cluster scaling. DynamoDB also offers encryption at rest, which eliminates the operational burden and complexity involved in protecting sensitive data.\nThis cheat sheet will cover the most commonly used scenarios of data operations in DynamoDB with AWS DynamoDB Document client for JavaScript/Nodejs.","title":"DynamoDB CheatSheet For NodeJS/JavaScript"},{"content":"Load testing is an important part when you are designing any type of application, whether it is traditional EC2 based or container-based or a complete serverless application.\nWhy is Load Testing important? Load testing will help us to find the following\n- How fast is the system\n- How much load can the system handle\n- Under what conditions will the system fail\n- Determine our application‚Äôs capabilities by measuring its response time, throughput, CPU utilization, latency, etc. during average and heavy user load. This will eventually help in determining the infrastructure needs as the system scales upward.\n- It gives us an opportunity to find strange behavior or surprises when we subject an application to an insane amount of load (stress testing). Strange behaviors include request timeouts, IO Exceptions, memory leaks, or any security issues.\nChoosing a Load testing tool or framework There are many great load testing frameworks available. Some of the leading tools are,\n- Jmeter\n- Locust\n- Artillery.io\nEach of the above tools provides common and some additional features and different methods of load testing. But the only problem with these tools is the throughput it can generate towards your application is limited to the host systems' memory and CPU capacity. If you want to want to test high and quick traffic ramp-up scenarios it\u0026rsquo;s not possible to do it from your laptop or PC. You can either have a high-end PC or you can run it on a Cloud Virtual Machine, it can be expensive, plus some of the above tools come with a GUI, which cannot be accessed via VM\u0026rsquo;s.\nSo how can we do load tests at scale without having a high-end testing infrastructure?\nLoad Testing Serverless Applications with Serverless Artillery Serverless artillery is a combination of serverless framework and artillery.io\nCombine serverless with artillery and you get serverless-artillery for an instant, cheap, and easy performance testing at scale\nServerless-artillery makes it easy to test your services for performance and functionality quickly, easily, and without having to maintain any servers or testing infrastructure.\nUse serverless-artillery if 1. You want to know if your services (either internal or public) can handle different amounts of traffic load (i.e. performance or load testing).\n2. You want to test if your services behave as you expect after you deploy new changes (i.e. acceptance testing).\n3. You want to constantly monitor your services overtime to make sure the latency of your services is under control (i.e. monitoring mode).\nHow It Works - Serverless-artillery would be installed and run on your local machine. From the command line run slsart --help to see various serverless-artillery commands\n- It takes your JSON or YAML load script `script.yml` that specifies,\n- test target/URL/endpoint/service - load progression - and the scenarios that are important for your service to test.  Let\u0026rsquo;s See It in Action *Load Testing A Sample Application*\nIn this example, we will load test a single endpoint(GET) serverless API built with AWS API Gateway, Lambda, and DynamoDB\n*Installing Serverless Artillery on local machine*\n*Prerequisite*\n- NodeJS v8 +\n- Serverless Framework CLI\nnpm install -g serverless  Installing serverless-artillery\nnpm install -g serverless-artillery  To check that the installation succeeded, run:\nslsart --version  We can also install it on a [docker container](https://github.com/Nordstrom/serverless-artillery#installing-in-docker)\n*Setting up the Load Test Configuration*\nmkdir load-test cd load-test slsart script // this will create script.yml config: target: \u0026quot;https://xxxxxxx.execute-api.us-east-1.amazonaws.com\u0026quot; phases: - duration: 300 arrivalRate: 500 rampTo: 10000 scenarios: - flow: - get: url: \u0026quot;/dev/get?id=john\u0026quot;  Understanding `script.yml`\nconfig:\nThe config section defines the target (the hostname or IP address of the system under test),the load progression, and protocol-specific settings such as HTTP response timeouts or [Socket.io](http://socket.io/) transport options\ntarget:\nthe URI of the application under test. For an HTTP application, it\u0026rsquo;s the base URL for all requests\nphases:\nspecify the duration of the test and the frequency of requests\nscenarios:\nThe scenarios section contains definitions for one or more scenarios for the virtual users that Artillery will create.\nflow:\na \u0026ldquo;flow\u0026rdquo; is an array of operations that a virtual user performs, e.g. GET and POST requests for an HTTP-based application\n*Deploy to AWS*\nslsart deploy --stage \u0026lt;your-unique-stage-name\u0026gt;  Start the load Test\nslsart invoke --stage \u0026lt;your-unique-stage-name\u0026gt;  The above \u0026ldquo;script.yml\u0026rdquo; will try to generate 500 user request/second towards the API Gateway Endpoint and it will try to ramp up the requests to 10000/RPS in a period of 5 minutes\nAnd the result of the test will look like this in a cloud watch dashboard.\nAs we can see in the above graph, there are a lot of requests that were throttled by lambda. That is because of lambda\u0026rsquo;s concurrency limit of 1000.\nHow Load Testing Helps Serverless Applications One of the important insights we can get from load testing serverless applications is, It helps to find out the default soft limits or hidden limits of serverless tools. By knowing this we will be able to architecture our application to handle high traffic without throttling the request and hitting the AWS limits.\nIt also helps to find out the following things,\n- Lambda Insights\n- To find concurrency limits - To find out the timeouts - To find out Memory Exceptions - To find out Cold starts (You can warm up or add provisioned concurrency to those functions)  - API Gateway\n- To understand the request throttling limits, increase or decrease them according to application needs  - DynamoDB\n- To get the read write usage metrics and do capacity planning for handling different level of traffic ","permalink":"https://imewish.github.io/posts/load-testing-serverless-sls-artillery/","summary":"Load testing is an important part when you are designing any type of application, whether it is traditional EC2 based or container-based or a complete serverless application.\nWhy is Load Testing important? Load testing will help us to find the following\n- How fast is the system\n- How much load can the system handle\n- Under what conditions will the system fail\n- Determine our application‚Äôs capabilities by measuring its response time, throughput, CPU utilization, latency, etc.","title":"Load Testing Serverless Applications With Serverless Artillery"},{"content":"Hosting a static website with S3 is awesome! It is Faster, Cheaper, Zero maintenance.\nIn this article, we will see how to do URL redirects on a website hosted with AWS S3 and Cloudfront.\nThere was a scenario which I was faced once in my company, One of our websites had deleted some old content and replaced it with new content and URL. And when people who google search for that particular content they get the old URL which doest exists.\nTo fix this issue the approach we had was to do add a temporary redirect for that old URL to the new one until it gets updated at google search.\nThe Fix\nAWS S3 Static hosting provides an option to add redirection rules to the website hosted in a particular bucket. https://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-page-redirect.html\nIn this particular case, the URL\u0026rsquo;s we are going to use will be these,\n_https://example.com/content/old-content_\nand we will be redirecting this to\n_https://example.com/content/new/content_\nTo add the rules,\n Click on your bucket Go to properties and click on static website hosting Under the redirection rules filed, put the following code  Redirect Rule,\n\u0026lt;RoutingRules\u0026gt; \u0026lt;RoutingRule\u0026gt; \u0026lt;Condition\u0026gt; \u0026lt;KeyPrefixEquals\u0026gt;content/old-content/\u0026lt;/KeyPrefixEquals\u0026gt; \u0026lt;/Condition\u0026gt; \u0026lt;Redirect\u0026gt; \u0026lt;HostName\u0026gt;example.com\u0026lt;/HostName\u0026gt; \u0026lt;ReplaceKeyPrefixWith\u0026gt;content/new/content\u0026lt;/ReplaceKeyPrefixWith\u0026gt; \u0026lt;/Redirect\u0026gt; \u0026lt;/RoutingRule\u0026gt; \u0026lt;/RoutingRules\u0026gt;  Please note, The HostName(Line 7) part is important if your S3 website is configured with Cloudfront. Else during redirect, the domain name will be replaced with the S3 website endpoint.\nThat\u0026rsquo;s it. Now any requests coming to the old URL will be automatically redirected to the new one\n","permalink":"https://imewish.github.io/posts/2019-12-15-url-redirects-with-aws-s3-and-cloudfront/","summary":"Hosting a static website with S3 is awesome! It is Faster, Cheaper, Zero maintenance.\nIn this article, we will see how to do URL redirects on a website hosted with AWS S3 and Cloudfront.\nThere was a scenario which I was faced once in my company, One of our websites had deleted some old content and replaced it with new content and URL. And when people who google search for that particular content they get the old URL which doest exists.","title":"URL redirects with AWS S3 and Cloudfront"},{"content":"In this guide we will set up a very simple REST API endpoint with the serverless framework, AWS Lambda, and API Gateway and deploy it to AWS Lambda with Github, AWS Codepipeline, Codebuild\n1. Install the Serverless Framework npm install serverless -g 2. Create a project serverless create --template aws-nodejs --path serverless-nodejs-api This will create two files handler.js and serveless.yml\n'use strict'; module.exports.api = async event =\u0026gt; { return { statusCode: 200, body: JSON.stringify( { message: 'Go Serverless v1.0! Your function executed successfully!' }, null, 2 ), }; }; Update your serverless.yml to add an API Gateway endpoint.\nservice: serverless-nodejs-api provider: name: aws runtime: nodejs10.x stage: dev functions: getMsg: handler: handler.api events: - http: GET / Now we have our serverless API code ready.\nYou can deploy this to AWS manually by running sls deploy --stage dev\nThis will deploy the lambda function and create an API gateway endpoint for the function.\nOnce deployed, the output will print the newly created API gateway endpoint. test the function by calling the API endpoint. Something like this,\nService Information service: serverless-nodejs-api stage: dev region: us-east-1 stack: serverless-nodejs-api-dev resources: 9 api keys: None endpoints: GET - https://xxxxx.execute-api.us-east-1.amazonaws.com/dev functions: api: serverless-nodejs-api-dev-getMsg layers: None test the function by calling the API endpoint.\ncurl https://xxxxx.execute-api.us-east-1.amazonaws.com/dev { \u0026quot;message\u0026quot;: \u0026quot;Go Serverless v1.0! Your function executed successfully!\u0026quot; } Now let\u0026rsquo;s automate the deployment process with Github, AWS Codepipeline\nLet\u0026rsquo;s consider this code as production-ready and push the code to the GitHub repo master branch.\nPS: We can create multiple pipelines per brach for eg: Master -\u0026gt; Prod, Development -\u0026gt; Staging/Dev Environment\n3. Setup Codepipeline 3.1 Set Pipeline name and Create IAM Role  3.2 Add source stage In this stage, Connect to your Github account and choose your repo and branch Set the detection method\n3.3 Add build stage In this step, we have to create a Codebuild project, where we configure our build and deploy environment and commands.\nClick on the Create Project button, it will take you to the Codebuild setup page.\nSet the project name here\nChoose your runtime and image for the build environment\nChoose an IAM role for the project - This part is important\nThis role must have enough permissions for the serverless framework to deploy the function and its resources to AWS as follows,\n Create an S3 bucket for your function deployments Upload your function zip files to that S3 bucket Submit a CloudFormation template Create the log groups for your Lambda functions Create a REST API in API Gateway  You can use the below awesome NPM modules to create a narrow IAM policy template that will cover many Serverless use cases.\nnpm install -g yo generator-serverless-policy\nthen on your serverless app directory\n$ yo serverless-policy ? Your Serverless service name test-service ? You can specify a specific stage, if you like: dev ? You can specify a specific region, if you like: us-west-1 ? Does your service rely on DynamoDB? Yes ? Is your service going to be using S3 buckets? Yes app name test-service app stage dev app region us-west-1 Writing to test-service-dev-us-west-1-policy.json After you finish creating the codebuild project go to its IAM role and append the policy with the rules created by the above template.\nYou can find the IAM policy we used for this guide here, https://github.com/imewish/serverless-nodejs-api/blob/master/codebuild-IAM-policy.json\nDefine Build Spec.\nYou can find it here. https://github.com/imewish/serverless-nodejs-api/blob/master/buildspec.yml\n Here we will define the commands to set up the serverless framework and deploy commands to AWS.\nOn install phase\n  Set nodejs 10 as runtime\n  Install serverless framework On Build Phase\n  Install npm packages\n  Deploy to lambda with sls deploy --stage dev/prod\n   NB: You can also run your tests here if you have test cases written for your lambda functions.\nEnable Cloudwatch logs so that we can tail our build process logs.\nThen click on Continue to Codepipeline this will take us back to Codepipeline Setup.\n4. Deploy Stage This stage is optional.\nSince the serverless framework already put the deployment artifacts to an S3 bucket we can skip this part. But if you want to store it to a different bucket you can set up like this.\nClick Next and then review all the setup then Create the pipeline.\nThat\u0026rsquo;s it!. Now you can test this by going to the newly created pipeline and click on Release Change\n","permalink":"https://imewish.github.io/posts/2019-11-23-automating-deployment-of-lambda-functions-using-serverless-framework-aws-codepipeline/","summary":"In this guide we will set up a very simple REST API endpoint with the serverless framework, AWS Lambda, and API Gateway and deploy it to AWS Lambda with Github, AWS Codepipeline, Codebuild\n1. Install the Serverless Framework npm install serverless -g 2. Create a project serverless create --template aws-nodejs --path serverless-nodejs-api This will create two files handler.js and serveless.yml\n'use strict'; module.exports.api = async event =\u0026gt; { return { statusCode: 200, body: JSON.","title":"Automating Deployment Of Lambda Functions Using Serverless Framework, AWS CodePipeline"},{"content":"An AWS and Serverless enthusiast, Experienced Serverless Developer/DevOps Engineer with a demonstrated history of working in the OTT and E-Commerce Industries.\n","permalink":"https://imewish.github.io/about-hugo/","summary":"An AWS and Serverless enthusiast, Experienced Serverless Developer/DevOps Engineer with a demonstrated history of working in the OTT and E-Commerce Industries.","title":"About Me"}]